**KISA AI Security êµìœ¡ê³¼ì •ì€ ì•„ë‹ˆì§€ë§Œ, ~~ì»¤ë¦¬í˜ëŸ¼ì—~~ ì œì‹¤ë ¥ì— ë¶€ì¡±í•œ ë¶€ë¶„ì´ ë§ì´ìˆì–´ ë…í•™í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤**  
**ğŸ‘¹ê³µë¶€ì¤‘ ë”°ë¼í•´ë³´ê³  ë‹¤ìŒì£¼ê¹Œì§€ ë§ˆë¬´ë¦¬í•˜ê¸°ğŸ‘¹**  
  
# <ê¸°ê³„í•™ìŠµ ê°œë¡ > ì‹¤ìŠµ
ì´í˜„ë„ â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
ìµœì›ì„ â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
ê¹€ìœ¤ì„± â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
https://bi.snu.ac.kr/Courses/ML2019/lab_slides/Lab1.pdf  

â—¼ ì‹¤ìŠµ ì½”ë“œ Github ì£¼ì†Œ â—¼ https://github.com/illhyhl1111/SNU_ML2019 â—¼  
-----
ì…€ìƒì„± crtl + M + A  
ì…€ì‚­ì œ ctrl + M + D  
ì…€ì¤‘ë‹¨ ctrl + M + L  
ìˆ˜ì •â–¶ë…¸íŠ¸ì„¤ì •â–¶í•˜ë“œì›¨ì–´ê°€ì†ê¸°â–¶GPU/TPU  
Tensor.cuda() x.cuda() val.cuda() GPU/TPU ì‚¬ìš©  
Tensor() x() val() CPU ì‚¬ìš©  
-----
â—¼ Pytorchë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ í”„ë¡œê·¸ë¨ì˜ ê¸°ë³¸ êµ¬ì¡°  
- Model(nn.Module) : Class  
  - ì „ì²´ì ì¸ ì¸ê³µì‹ ê²½ë§ì˜ êµ¬ì¡°ë¥¼ ì„ ì–¸í•˜ëŠ” ë¶€ë¶„  
  -     __ init __ , forward ë“±ì˜ methodëŠ” ê¼­ í•„ìš”í•¨  
- Dataset : Class   
  - í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ì…‹ì„ ì„ ì–¸í•˜ëŠ” ë¶€ë¶„  
  -     __ init __, __ len __, __ getitem __ì˜ methodëŠ” ê¼­ í•„ìš”í•¨  
- Training   
  - í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë¶€ë¶„  
  -     optimizer(model, learning_rate)  
  -     for epoch in range:  
  -     optimizer.zero();  
    - __ê° batchë§ˆë‹¤ gradient ê°’ì„ ì´ˆê¸°í™”__  
  -     loss_function(model);  
  -     loss.backward();  
    - __ê³„ì‚°ëœ tensorì— ëŒ€í•´ ì—­ìœ¼ë¡œ ê³„ì‚°í•˜ë©° gradientë¥¼ êµ¬í•¨__
  -     optmizer.step();  
    - __gradientë¥¼ í™œìš©í•˜ì—¬ backpropagationì„ ìˆ˜í–‰, ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ update__   
- {Validation & Test}  
  - í•™ìŠµëœ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ë¶€ë¶„ 
- LEARNING_RATE
  - í•œë²ˆì˜ í•™ìŠµì„ í†µí•´ ë³€í™”ì‹œí‚¤ëŠ” ì •ë„
  - ë¬¸ì œë§ˆë‹¤ ì ì ˆí•œ ê°’ ì„¤ì • í•„ìš”
- BATCH_SIZE
  - í•œë²ˆì— ì „ì²´ ë°ì´í„°ë¥¼ ë„£ëŠ” ê²ƒì€ í¬ê¸°ê°€ ë„ˆë¬´ í´ ë¿ ì•„ë‹ˆë¼, ì¢‹ì§€ ì•ŠìŒ
  - Batchì˜ í¬ê¸°ë¥¼ ì„¤ì •í•˜ì—¬ random sampling
- NUM_EPOCHES
  - ì „ì²´ ë°˜ë³µí•  ì´ epochì˜ ìˆ˜
  - 1 epoch == ë°ì´í„°ì…‹ì˜ í•œ iter
-----
â—¼ MLP ëª¨ë¸ì˜ êµ¬ì„±  
- nn.Linear(in_features, out_features)  
  - Weight matrix, bias vectorë¥¼ í¬í•¨í•œ ë‹¨ì¼ perceptron layerë¥¼ êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
    - Input, outputê°œìˆ˜: in_features, out_features
  - nn.Moduleì„ inherití•œ ì¼ì¢…ì˜ submodule
  - Parameterë¥¼ ê°€ì§€ë¯€ë¡œ ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì„ ì–¸ í•„ìš”
- nn.functional.relu(input)  
  - ReLu activation function
- torch.sigmoid(input)  
  - ê²°ê³¼ê°’ shapingì„ ìœ„í•œ Sigmoid function
  - ëª¨ë¸ì´ ì•„ë‹Œ ë‹¨ìˆœí•œ í•¨ìˆ˜ì´ë¯€ë¡œ ì´ˆê¸°í™” í•„ìš” ì—†ìŒ
-----
# Model initialization  
â—¼ ëª¨ë¸ ì„ ì–¸  
- __ init __(self)
  - 2ê°œì˜ fully connected(linear) layerë¥¼ ì„ ì–¸
    - Weight initialization
  - ë…¸ë“œ ê°œìˆ˜: 2(x,y)â†’4(hidden) â†’ 1(val_)
- forward(self, x) 
  - val_ = sigmoid(fc2(ReLu(fc1(x)))  
 
# Training phase  
- Hyperparameter ì„ ì–¸
  - batch_size
  - num_epochs
  - learning_rate
- Optimizer ì´ˆê¸°í™”
- ëª¨ë¸, ë°ì´í„°ì…‹ ì´ˆê¸°í™”
- for each epoch
