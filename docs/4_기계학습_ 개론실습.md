**KISA AI Security 교육과정은 아니지만, ~~커리큘럼에~~ 제실력에 부족한 부분이 많이있어 독학하는 부분입니다**  
**👹공부중 따라해보고 다음주까지 마무리하기👹**  
  
# <기계학습 개론> 실습
이현도 ‖ 서울대학교 Biointelligence Laboratory  
최원석 ‖ 서울대학교 Biointelligence Laboratory  
김윤성 ‖ 서울대학교 Biointelligence Laboratory  
https://bi.snu.ac.kr/Courses/ML2019/lab_slides/Lab1.pdf  

◼ 실습 코드 Github 주소 ◼ https://github.com/illhyhl1111/SNU_ML2019 ◼  
-----
셀생성 crtl + M + A  
셀삭제 ctrl + M + D  
셀중단 ctrl + M + L  
수정▶노트설정▶하드웨어가속기▶GPU/TPU  
Tensor.cuda() x.cuda() val.cuda() GPU/TPU 사용  
Tensor() x() val() CPU 사용  
-----
◼ Pytorch를 활용한 딥러닝 프로그램의 기본 구조  
- Model(nn.Module) : Class  
  - 전체적인 인공신경망의 구조를 선언하는 부분  
  -     __ init __ , forward 등의 method는 꼭 필요함  
- Dataset : Class   
  - 학습에 필요한 데이터셋을 선언하는 부분  
  -     __ init __, __ len __, __ getitem __의 method는 꼭 필요함  
- Training   
  - 학습을 진행하는 부분  
- {Validation & Test}  
  - 학습된 결과를 확인하는 부분 
- LEARNING_RATE
  - 한번의 학습을 통해 변화시키는 정도
  - 문제마다 적절한 값 설정 필요
- BATCH_SIZE  
  - 한번에 처리할 데이터묶음
  - 한번에 전체 데이터를 넣는 것은 크기가 너무 클 뿐 아니라, 좋지 않음
  - Batch의 크기를 설정하여 random sampling  
  - L2 loss가 0 또는 1이 되도록 진동  
  -     Minibatch ◼ 한번에 10개의 x에 대한 loss를 구하고 평균내어 batch 단위의 L2 loss를 구함. 0.8로 수렴  
  - 크기가 커질수록 ◼ 안정적인 학습 가능 ◼ 학습 시간 대비 효율 감소 ◼ 많은 메모리 소모  
- NUM_EPOCHES
  - 전체 반복할 총 epoch의 수
  - 1 epoch == 데이터셋의 한 iter
-----
◼ MLP 모델의 구성  
- nn.Linear(in_features, out_features)  
  - Weight matrix, bias vector를 포함한 단일 perceptron layer를 구현한 라이브러리
    - Input, output개수: in_features, out_features
  - __nn.Module을 inherit한 일종의 submodule__
  - __Parameter를 가지므로 모델 초기화 시 선언 필요__
- nn.functional.relu(input)  
  - ReLu activation function
- torch.sigmoid(input)  
  - 결과값 shaping을 위한 Sigmoid function
  - 모델이 아닌 단순한 함수이므로 초기화 필요 없음
-----
# Model initialization  
◼ 모델 선언  
- __ init __(self)
  - 2개의 fully connected(linear) layer를 선언
    - Weight initialization
  - 노드 개수: 2(x,y)→4(hidden) → 1(val_)
- forward(self, x)  
  - val_ = sigmoid(fc2(ReLu(fc1(x)))  
  - return x == 1(val_)  
 
# Training phase  
- Hyperparameter 선언
  - batch_size, num_epochs, learning_rate
- Optimizer 초기화
- 모델, 데이터셋 초기화
- for each epoch
  -     optimizer = Adam(model, learning_rate)  
    - Adam optimizer를 많이 사용  
  -     for epoch in range:  
  -     optimizer.zero();  
    - __각 batch마다 gradient 값을 초기화__  
  -     val_ = model(x);  
  -     loss_function(model);  
    - loss_function(val, val_);  
    - 모델의 결과값과 실제 정답과의 차이
    - 패널티
    - L2-norm loss: sqrt(𝑣𝑎𝑙 − 𝑣𝑎𝑙)^2   
  -     loss.backward();  
    - __계산된 tensor에 대해 역으로 계산하며 gradient를 구함__  
    - __실행 시 loss와 연결된 모든 tensor의 .grad 값이 계산됨__  
  -     optmizer.step();  
    - __gradient를 활용하여 backpropagation을 수행, 가중치 벡터를 update__   
    - 학습할 Variable 리스트에 포함된 변수들의 값 변경  
    - 변경 방법은 optimizer 종류 따라 다름  
