**KISA AI Security êµìœ¡ê³¼ì •ì€ ì•„ë‹ˆì§€ë§Œ, ~~ì»¤ë¦¬í˜ëŸ¼ì—~~ ì œì‹¤ë ¥ì— ë¶€ì¡±í•œ ë¶€ë¶„ì´ ë§ì´ìˆì–´ ë…í•™í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤**  
**ğŸ‘¹ê³µë¶€ì¤‘ ë”°ë¼í•´ë³´ê³  ë‹¤ìŒì£¼ê¹Œì§€ ë§ˆë¬´ë¦¬í•˜ê¸°ğŸ‘¹**  
  
# <ê¸°ê³„í•™ìŠµ ê°œë¡ > ì‹¤ìŠµ
ì´í˜„ë„ â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
ìµœì›ì„ â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
ê¹€ìœ¤ì„± â€– ì„œìš¸ëŒ€í•™êµ Biointelligence Laboratory  
https://bi.snu.ac.kr/Courses/ML2019/lab_slides/Lab1.pdf  

â—¼ ì‹¤ìŠµ ì½”ë“œ Github ì£¼ì†Œ â—¼ https://github.com/illhyhl1111/SNU_ML2019 â—¼  
-----
ì…€ìƒì„± crtl + M + A  
ì…€ì‚­ì œ ctrl + M + D  
ì…€ì¤‘ë‹¨ ctrl + M + L  
ìˆ˜ì •â–¶ë…¸íŠ¸ì„¤ì •â–¶í•˜ë“œì›¨ì–´ê°€ì†ê¸°â–¶GPU/TPU  
Tensor.cuda() x.cuda() val.cuda() GPU/TPU ì‚¬ìš©  
Tensor() x() val() CPU ì‚¬ìš©  
-----
â—¼ Pytorchë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ í”„ë¡œê·¸ë¨ì˜ ê¸°ë³¸ êµ¬ì¡°  
- Model(nn.Module) : Class  
  - ì „ì²´ì ì¸ ì¸ê³µì‹ ê²½ë§ì˜ êµ¬ì¡°ë¥¼ ì„ ì–¸í•˜ëŠ” ë¶€ë¶„  
  -     __ init __ , forward ë“±ì˜ methodëŠ” ê¼­ í•„ìš”í•¨  
- Dataset : Class   
  - í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ì…‹ì„ ì„ ì–¸í•˜ëŠ” ë¶€ë¶„  
  -     __ init __, __ len __, __ getitem __ì˜ methodëŠ” ê¼­ í•„ìš”í•¨  
- Training   
  - í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë¶€ë¶„  
- {Validation & Test}  
  - í•™ìŠµëœ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ë¶€ë¶„ 
- LEARNING_RATE
  - í•œë²ˆì˜ í•™ìŠµì„ í†µí•´ ë³€í™”ì‹œí‚¤ëŠ” ì •ë„
  - ë¬¸ì œë§ˆë‹¤ ì ì ˆí•œ ê°’ ì„¤ì • í•„ìš”
- BATCH_SIZE  
  - í•œë²ˆì— ì²˜ë¦¬í•  ë°ì´í„°ë¬¶ìŒ
  - í•œë²ˆì— ì „ì²´ ë°ì´í„°ë¥¼ ë„£ëŠ” ê²ƒì€ í¬ê¸°ê°€ ë„ˆë¬´ í´ ë¿ ì•„ë‹ˆë¼, ì¢‹ì§€ ì•ŠìŒ
  - Batchì˜ í¬ê¸°ë¥¼ ì„¤ì •í•˜ì—¬ random sampling  
  - L2 lossê°€ 0 ë˜ëŠ” 1ì´ ë˜ë„ë¡ ì§„ë™  
  -     Minibatch â—¼ í•œë²ˆì— 10ê°œì˜ xì— ëŒ€í•œ lossë¥¼ êµ¬í•˜ê³  í‰ê· ë‚´ì–´ batch ë‹¨ìœ„ì˜ L2 lossë¥¼ êµ¬í•¨. 0.8ë¡œ ìˆ˜ë ´  
  - í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ â—¼ ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥ â—¼ í•™ìŠµ ì‹œê°„ ëŒ€ë¹„ íš¨ìœ¨ ê°ì†Œ â—¼ ë§ì€ ë©”ëª¨ë¦¬ ì†Œëª¨  
- NUM_EPOCHES
  - ì „ì²´ ë°˜ë³µí•  ì´ epochì˜ ìˆ˜
  - 1 epoch == ë°ì´í„°ì…‹ì˜ í•œ iter
-----
â—¼ MLP ëª¨ë¸ì˜ êµ¬ì„±  
- nn.Linear(in_features, out_features)  
  - Weight matrix, bias vectorë¥¼ í¬í•¨í•œ ë‹¨ì¼ perceptron layerë¥¼ êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
    - Input, outputê°œìˆ˜: in_features, out_features
  - __nn.Moduleì„ inherití•œ ì¼ì¢…ì˜ submodule__
  - __Parameterë¥¼ ê°€ì§€ë¯€ë¡œ ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì„ ì–¸ í•„ìš”__
- nn.functional.relu(input)  
  - ReLu activation function
- torch.sigmoid(input)  
  - ê²°ê³¼ê°’ shapingì„ ìœ„í•œ Sigmoid function
  - ëª¨ë¸ì´ ì•„ë‹Œ ë‹¨ìˆœí•œ í•¨ìˆ˜ì´ë¯€ë¡œ ì´ˆê¸°í™” í•„ìš” ì—†ìŒ
-----
# Model initialization  
â—¼ ëª¨ë¸ ì„ ì–¸  
- __ init __(self)
  - 2ê°œì˜ fully connected(linear) layerë¥¼ ì„ ì–¸
    - Weight initialization
  - ë…¸ë“œ ê°œìˆ˜: 2(x,y)â†’4(hidden) â†’ 1(val_)
- forward(self, x)  
  - val_ = sigmoid(fc2(ReLu(fc1(x)))  
  - return x == 1(val_)  
 
# Training phase  
- Hyperparameter ì„ ì–¸
  - batch_size, num_epochs, learning_rate
- Optimizer ì´ˆê¸°í™”
- ëª¨ë¸, ë°ì´í„°ì…‹ ì´ˆê¸°í™”
- for each epoch
  -     optimizer = Adam(model, learning_rate)  
    - Adam optimizerë¥¼ ë§ì´ ì‚¬ìš©  
  -     for epoch in range:  
  -     optimizer.zero();  
    - __ê° batchë§ˆë‹¤ gradient ê°’ì„ ì´ˆê¸°í™”__  
  -     val_ = model(x);  
  -     loss_function(model);  
    - loss_function(val, val_);  
    - ëª¨ë¸ì˜ ê²°ê³¼ê°’ê³¼ ì‹¤ì œ ì •ë‹µê³¼ì˜ ì°¨ì´
    - íŒ¨ë„í‹°
    - L2-norm loss: sqrt(ğ‘£ğ‘ğ‘™ âˆ’ ğ‘£ğ‘ğ‘™)^2   
  -     loss.backward();  
    - __ê³„ì‚°ëœ tensorì— ëŒ€í•´ ì—­ìœ¼ë¡œ ê³„ì‚°í•˜ë©° gradientë¥¼ êµ¬í•¨__  
    - __ì‹¤í–‰ ì‹œ lossì™€ ì—°ê²°ëœ ëª¨ë“  tensorì˜ .grad ê°’ì´ ê³„ì‚°ë¨__  
  -     optmizer.step();  
    - __gradientë¥¼ í™œìš©í•˜ì—¬ backpropagationì„ ìˆ˜í–‰, ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ update__   
    - í•™ìŠµí•  Variable ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë³€ìˆ˜ë“¤ì˜ ê°’ ë³€ê²½  
    - ë³€ê²½ ë°©ë²•ì€ optimizer ì¢…ë¥˜ ë”°ë¼ ë‹¤ë¦„  
