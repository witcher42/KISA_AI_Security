**KISA AI Security 교육과정은 아니지만, ~~커리큘럼에~~ 제실력에 부족한 부분이 많이있어 독학하는 부분입니다**  
  
# <기계학습 개론> 실습
이현도 ‖ 서울대학교 Biointelligence Laboratory  
최원석 ‖ 서울대학교 Biointelligence Laboratory  
김윤성 ‖ 서울대학교 Biointelligence Laboratory  
https://bi.snu.ac.kr/Courses/ML2019/lab_slides/Lab1.pdf  
# Machine Learning (2019 Fall)  
https://bi.snu.ac.kr/Courses/ML2019/ML2019.html  
서울대학교 | 장병탁
📚📕📖  

◼ 실습 코드 Github 주소 ◼ https://github.com/illhyhl1111/SNU_ML2019 ◼   
-----

## 딥러닝 : 모델의 복잡도가 큰 머신러닝  
- 특징 맵(feature map) 또는 표상(representation)을 자동으로 학습  
- LMS 최소 평균제곱오차법 : 오류 함수를 경사하강법을써서 반복적으로 최소화하는 것 
### 다층퍼셉트론 MLP 
- 퍼셉트론 + 은닉층
### 오류역전파 알고리즘 BP
- 상위층의 델타값을 이용한 하위층 뉴런의 델타값 계산
- but 학습속도가 느림, 국지적 최소점만 찾음 
### 경사도 소멸 문제 (vanishing gradient)  
- 깊은 신경망 학습시 문제점
- 오류 역전파 알고리듬으로 깊이가 깊은 심층신경망 학습시에 문제가 발생
- 역전파 오류가 입력층으로 전달됨에 따라 그래디언트가 작아지게 됨
- 가중치 매개변수가 업데이트 되지 않는 경우가 발생하여 학습이 잘 안됨
### 오차 신호 희석 문제 (gradient vanishing problem)
- Sigmoid는 순환신경망에서 사용되면 근방의 정보만 반영하여 학습하여 데이터의 순서를 학습하지 못함
- 오차 신호 희석 문제를 해결하기 위해서 ReLU를 활성화 함수로 활용  
- 해결
  - 장단기 메모리(Long-Short Term Memory, LSTM) 신경망 모델
    - Gate를 도입하여 cell state를 제어
    -  LSTM 내부 구조에는 sigmoid, tanh가 들어있음  
### 품사 추론 LSTM 모델
- 문장이 주어지면, 각 단어의 품사를 추론하는 모델 설계  
- 이전 단어들을 기억할 수 있는 LSTM 모델이 적합
### Word2Vec
- 한 문장에 있는 단어들이 similar context라 간주

### 컨볼루션 커널 
- 구조 설계를 통해 모델 복잡도를 줄임  
-----
셀생성 crtl + M + A  
셀삭제 ctrl + M + D  
셀중단 ctrl + M + L  
수정▶노트설정▶하드웨어가속기▶GPU/TPU  
Tensor.cuda() x.cuda() val.cuda() GPU/TPU 사용  
Tensor() x() val() CPU 사용  
학습된 파라메터만을 디스크에 저장 (recommended)  
◼ 대상: model.state_dict()  
◼ Save: torch.save(model.state_dict(), dir)  
◼ Load: model.load_state_dict(torch.load(dir))  

-----
◼ Pytorch를 활용한 딥러닝 프로그램의 기본 구조  
- Model(nn.Module) : Class  
  - 전체적인 인공신경망의 구조를 선언하는 부분  
  -     __ init __ , forward 등의 method는 꼭 필요함  
- Dataset : Class   
  - 학습에 필요한 데이터셋을 선언하는 부분  
  -     __ init __, __ len __, __ getitem __의 method는 꼭 필요함  
- Training   
  - 학습을 진행하는 부분  
- {Validation & Test}  
  - 학습된 결과를 확인하는 부분 
  - __학습 기간을 지나치게 오래 두면 overfitting 등의 문제가 발생__  
  - Test set은 모델의 최종 평가만을 위한 데이터셋, 학습 과정에 관여해선 안됨  
  - Validation set의 성능이 떨어지면 overfitting  
- LEARNING_RATE
  - 한번의 학습을 통해 변화시키는 정도
  - 문제마다 적절한 값 설정 필요
- BATCH_SIZE  
  - 한번에 처리할 데이터묶음
  - 한번에 전체 데이터를 넣는 것은 크기가 너무 클 뿐 아니라, 좋지 않음
  - Batch의 크기를 설정하여 random sampling  
  - L2 loss가 0 또는 1이 되도록 진동  
  -     Minibatch ◼ 한번에 10개의 x에 대한 loss를 구하고 평균내어 batch 단위의 L2 loss를 구함. 0.8로 수렴  
  -     크기가 커질수록 ◼ 안정적인 학습 가능 ◼ 학습 시간 대비 효율 감소 ◼ 많은 메모리 소모  
  - Noise가 바뀐 test data에서 성능이 낮은 이유는?  
    - Training data의 noise값까지 모두 학습하였다!  
    - Batch normalization 필요  
    - Regularizer: coefficient의 값의 범위를 제한시켜 overfitting 방지  

- NUM_EPOCHES
  - 전체 반복할 총 epoch의 수
  - 1 epoch == 데이터셋의 한 iter
-----
◼ MLP 모델의 구성  
- nn.Linear(in_features, out_features)  
  - Weight matrix, bias vector를 포함한 단일 perceptron layer를 구현한 라이브러리
    - Input, output개수: in_features, out_features
  - __nn.Module을 inherit한 일종의 submodule__
  - __Parameter를 가지므로 모델 초기화 시 선언 필요__
- nn.functional.relu(input)  
  - ReLu activation function
- torch.sigmoid(input)  
  - 결과값 shaping을 위한 Sigmoid function
  - 모델이 아닌 단순한 함수이므로 초기화 필요 없음
-----
# Model initialization  
◼ 모델 선언  
- __ init __(self)
  - 2개의 fully connected(linear) layer를 선언
    - Weight initialization
  - 노드 개수: 2(x,y)→4(hidden) → 1(val_)
- forward(self, x)  
  - val_ = sigmoid(fc2(ReLu(fc1(x)))  
  - return x == 1(val_)  
 
# Training phase  
- Hyperparameter 선언
  - batch_size, num_epochs, learning_rate
- Optimizer 초기화
- 모델, 데이터셋 초기화
- for each epoch
  -     optimizer = Adam(model, learning_rate)  
    - Adam optimizer를 많이 사용  
  -     for epoch in range:  
  -     optimizer.zero();  
    - __각 batch마다 gradient 값을 초기화__  
  -     val_ = model(x);  
  -     loss_function(model);  
    - loss_function(val, val_);  
    - 모델의 결과값과 실제 정답과의 차이
    - 패널티
    - L2-norm loss: sqrt(𝑣𝑎𝑙 − 𝑣𝑎𝑙)^2   
  -     loss.backward();  
    - __계산된 tensor에 대해 역으로 계산하며 gradient를 구함__  
    - __실행 시 loss와 연결된 모든 tensor의 .grad 값이 계산됨__  
  -     optmizer.step();  
    - __gradient를 활용하여 backpropagation을 수행, 가중치 벡터를 update__   
    - 학습할 Variable 리스트에 포함된 변수들의 값 변경  
    - 변경 방법은 optimizer 종류 따라 다름  
