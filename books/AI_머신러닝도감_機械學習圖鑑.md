- 특징을 나타내는 데이터 : __feature__ 또는 __독립변수__ 또는 __입력변수__  
- 정답을 나타내는 데이터 : __label__ 또는 __종속변수__ 또는 __타겟__  
  -      사용자가 직접 스팸 메일을 판단한 데이터 = label(종속변수, 타겟)    
  -      이메일을 보내는 사람 또는 스팸으로 판단한 문장 = feature(독립변수)  
  -      🎃label(종속변수)로 선택한 데이터가 많을수록 머신러닝 정확도가 높다  
- __정규화__ : 과적합을 막는방법
  - 손실함수에 패널티를 추가해 과적합을 막는다  
  - 차수가 증가할때 검정오차가 커지지 않게해 과적합을 막는다  
- __로지스틱 회귀__ __Logistic Regression__  
  - 지도학습이면서 분류분석(회귀라는 단어는 fake!)  
  - 데이터가 집단 각각에 속하는 확률을 계산해 데이터를 분류한다.  
  - 선형회귀와 달리 확률을 계산하므로 출력결과의 범위를 0~1로 두어야 한다(기본과정 2주차에서 배운 sigmoid!)  
- __서포트 벡터 머신__ __SVM__  
  - 분류와 회귀에 모두 사용된다는 점  
  - 집단사이의 마진을 최대화한다는 점  
  - 서포트벡터 : 결정 경계를 정하는 데이터  
  - SVM에 커널기법을 적용하면 __사람이 직접 복잡한 데이터를 전처리해가며 특징을 만들 필요가 없다__  
  -      커널기법 : 데이터를 다른 특정 공간에 옮긴 후 선형회귀 하는 것  
- __나이브베이즈 분류__ __Naïve Bayes Classification__   
  - 자연어분류  
  - 입력데이터는 벡터로 변환해야한다  
  - 학습데이터로 각 레이블에서 단어가 나타날 확률을 학습한다  
  - 가장 확률이 높은 레이블을 분류 결과로 삼는다  
  -      스무딩 Smoothing : 표본이 매우 커지게된다면 0인 확률은 결국은 없는 것이므로 0.01과 같이 수정
- __랜덤 포레스트__ __Random Forest__  
  - 다수결 결정  
  - 데이터 불균형을 나타내는 불순도가([ex]지니계수) 작아지도록 데이터를 나눈다  
  -      {불순도 계산 --> 불순도가 가장 작아지도록 영역을 나눔} --> ∞  
- __KNN__  
  - 복잡한 결정 경계 학습  
  - 알수없는 레이블을 가진 데이터가 들어오면, 가까운 k개의 학습데이터간 거리를 계산하여 다수결로 데이터를 분류  
- __k-means__  
  - 클러스터링, 비지도학습  
  - 컴퓨터비전에 많이 사용됨  
  - 클러스터링 결과는 집단제곱합 WCSS로 평가한다   
# __**지도학습은 과적합 방지가 중요하다!**__   
- 과적합을 막는법  
  -      정규화  
  -      학습데이터와 검증데이터로 나누기  
  -      교차검증  
  -      앙상블 학습 : 여러 모델을 학습시킨 후 결합해 사용  
# __**지도학습 평가지표**__  
|분류문제|회귀문제|  
|:---:|:---:|
|Confusion Matrix|MSE(평균제곱오차)|
|Accuracy|결정계수|
|Precision|
|Recall(재현율)|
|F1 Score|
|AUC(곡선아래면적)|
- Confusion Matrix  
  - TN 예측 0 정답 0  
  - TP 예측 1 정답 1  
  - FN 예측 0 정답 1  
  - FP 예측 1 정답 0  
- Accuracy  
  - (TN+TP)/(TN+TP+FN+FP)  
  - 예측이 맞음 (긍정 or 부정)  
- Precision  
  - TP/(TP+FP)  
  - 긍정예측 중 정답긍정  
- Recall  
  - TP/(TP+FN)  
  - 정답긍정 중 예측정답긍정  
- F1 Score  
  - 2(prec. * rec.)/(prec. + rec.)  
  - 정밀도와 재현율은 이율 배반적인 관계에 있어서 한쪽을 늘리면 다른쪽이 낮아짐  
