 - 사이버 보안에 이용되는 AI 시스템이 증가하고, 더불어 발전되고 있찌만 새로운 형태의 공격을 위한 창구를 여는 새로운 취약성도 발견되고 있다  
 - 고전적인 소프트웨어 개발에서는 개발자가 컴퓨터 언어를 사용해 특정 알고리즘을 구현하는 프로그램을 만든다  
 - 이는 안전한 설계원칙을 사용할 수 있고, 취약점이 발생할 가능성이 있는 곳을 명확하게 파악할 수 있으며,  
 - 효과적인 보안통제를 실시할 수 있는 직선적인 개발프로세스를 구성한다  
-----
 - 이와는 대조적으로, AI 시스템은 특히 ML 아키텍쳐에 기반하는 경우에 기본적으로 더 복잡하다  
 - 이들은 실제의 'AI' 역할을 하며, 기본적으로 새로운 알고리즘인 학습모델을 구성한다  
 - 이러한 복잡한 시스템의 취약성 또는 설계상의 결함은 고전적인 시스템에서와는 매우 다른 성격을 가질 것이다  
 - 시스템에 대한 공격이나 취약성을 이용한 공격과 같이 이벤트와 그 원인을 추적하는 것은 훨씬 더 복잡한 형상이 되고, 부담이 큰 과제가 된다  
----- 
 - 설명 가능성은 기본연구의 핵심영역이다. 왜냐하면 우리는 AI 시스템의 성능을 증가시켜서 지식을 추상화하여 전달하고,  
 - 사이버 보안의 관점에서 AI 시스템의 견고성을 이해하며, 가능한 편견(bias)을 평가하고,  
 - 중요하게도 사회에 대한 신뢰를 구축하고 유지하기 위해 설명가능하며 해석가능한 모델이 필요하기 때문이다  
-----
 - 신흥 AI 보안 및 적대적 ML 계열은 이러한 새로운 과제에 적극적으로 관심을 기울이고 있다  
 # AI 시스템 특유의 대표적인 취약성은 다음과 같다 👻  
 
|데이터 중독|Data Poisoning. 모델 트레이닝에서의 허위 데이터의 고의적 도입은 특히 자율형 AI 시스템에 위험하다.|
|:--:|:--|
||이는 데이터 자체가 이러한 시스템의 주요 취약성이 된다는 것을 효과적으로 의미한다.|
|적대적 사례|Adversarial Examples. 사용중인 알고리즘 절차를 잘못 분류하거나 속이기 위해 의도적으로 설계된 ML을 시스템에 입력하는 것.|
|설계 결함|Design Flaws. 모델의 학습과정을 최적화하는 기반적인 산술절차에서의 설계 결함은 모델 공격을 유도한다.|
||예를 들어 특정 유형의 입력에 민감한 것으로 알려진 손실함수나 우도모델을 사용하면 적대적 사례를 위한 쉬운 경로를 만들 수 있다.|

# 적대적 ML에 관한 연구는 현재 공격의 의도와 취약성 유형과 같이 상황에 따른 2가지 주요 대책에 초점을 맞추고 있다  

|데이터 위생화|Data Sanitisation. 모델을 트레이닝하기 전에 잠재적인 모든 악성 콘텐츠의 트레이닝 데이터를 청소하는 것.|
|:--:|:--|
|강인학습|Robust Learning. 학습 절차를 악의적인 행위, 특히 적대적인 사례에 대항하여 강인하게 되도록 재설계하는 것.|
||설계 프로세스 초기부터 소프트웨어나 애플리케이션의 보안을 고려하나 성능 저하가 있다.|
