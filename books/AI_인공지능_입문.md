# ì¸ê³µì§€ëŠ¥ ì…ë¬¸ (2019 1í•™ê¸°)
https://bi.snu.ac.kr/Courses/IntroAI/IntroAI2019.html  
ì„œìš¸ëŒ€í•™êµ | ì¥ë³‘íƒ  
ğŸ“šğŸ“•ğŸ“–  
-----
- ë”¥ëŸ¬ë‹ì´ ì˜ ë‹¤ë£¨ëŠ” ë¬¸ì œ
  - í° ë¶„ëŸ‰ì˜ ë°ì´í„° í™•ë³´ê°€ ê°€ëŠ¥í•œ ë¬¸ì œ
    - ìƒë‹¹í•œ ë…¸ì´ì¦ˆê°€ ìˆì–´ë„ ë°ì´í„° ë¶„ëŸ‰ì´ í¬ë©´ ë‹¤ë£° ìˆ˜ ìˆìŒ
  - í‘œì§€(label)ê°€ ìˆëŠ” ë°ì´í„°
    - í˜„ì¬ ê¸°ìˆ  ìˆ˜ì¤€ì—ì„œëŠ” ì•„ì§ê¹Œì§€ëŠ” supervised learningì„ ë” ì˜í•¨
    - Labelì´ ì¼ë°˜ì ì¸ ë¶„ë¥˜ë¬¸ì œì˜ labelì¼ í•„ìš”ëŠ” ì—†ìŒ

- __ë¨¸ì‹ ëŸ¬ë‹__  
  -     ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°€ê³µì„ í†µí•´ ë¬¸ì œ í•´ê²°ì— ì í•©í•œ íŠ¹ì§• ì¶”ì¶œ í›„ ì´ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ íŒ¨í„´ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨
- __ë”¥ëŸ¬ë‹__  
  -     íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ (ë¬´ê°ë…í•™ìŠµ) ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— í¬í•¨ì‹œì¼œ ìë™ìœ¼ë¡œ í•™ìŠµí•¨  
-----
![title](../srcs/ë¨¸ì‹ ëŸ¬ë‹vsë”¥ëŸ¬ë‹.png) 
- __í™œì„±í™” í•¨ìˆ˜ (Activation function)__  
  - ê²½ê³„ë©´ì˜ ë¶€ê·¼ì˜ ëª¨ì–‘ì„ ê²°ì •  
  - s = wÂ·x   
    - ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (sigmoid function)  
    - í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜ (tanh function) 
    - ReLU (Rectified Linear function)  
- ë‹¤ì¸µ ë‰´ëŸ°ì˜ í•„ìš”ì„±   
  - ë³µì¡í•œ íŒ¨í„´ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œëŠ” ì…ì¶œë ¥ ê°„ì˜ ë³µì¡í•œ ë³€í™˜ êµ¬ì¡°ê°€ í•„ìš”  
  - ë‹¨ì¼ ë‰´ëŸ°ìœ¼ë¡œëŠ” ì„ í˜•ë¶„ë¦¬ê°€ ê°€ëŠ¥í•œ ê²°ì • ê²½ê³„ì„ ë§Œì„ ìƒì„± ê°€ëŠ¥  
  - ê° ì¸µ ì•ˆì—ì„œëŠ” ë‰´ëŸ°ê°„ ì—°ê²°ì´ ì—†ìœ¼ë©° ì¸ì ‘í•œ ë‘ ì¸µì˜ ë‰´ëŸ° ê°„ì—ëŠ” ì™„ì „ ì—°ê²°ë¨  
    - ì…ë ¥ì¸µ ë¹„ë¡€ì¡°ì •í•¨ìˆ˜   
    - ì€ë‹‰ì¸µ í™œì„±í•¨ìˆ˜   
    - ì¶œë ¥ì¸µ í™œì„±í•¨ìˆ˜  
- __ì „ë°©í–¥ ì¶”ë¡ (Forward	Chaining)__  
  - ë°ì´í„° --> ê·œì¹™  
  - ìƒí–¥ì‹ ì¶”ë¡ (bottom-up)  
  - ë°ì´í„°ê¸°ë°˜ ì¶”ë¡   
  - ê²°ê³¼ ì˜ˆì¸¡  
  - ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ë„ ìˆìœ¼ë‚˜ ìƒˆë¡œìš´ ê²°ê³¼ ë°œê²¬ ê°€ëŠ¥ì„± ìˆìŒ  
- __ì—­ë°©í–¥ ì¶”ë¡ (Backward	Chaining)__     
  - ê·œì¹™ --> ë°ì´í„°  
  - í•˜í–¥ì‹ ì¶”ë¡ (top-down)  
  - ê°€ì„¤ê¸°ë°˜ ì¶”ë¡   
  - ì›ì¸ ì§„ë‹¨  
  - ì¶”ë¡ ì´ íš¨ìœ¨ì ì´ë‚˜ ìƒˆë¡œìš´ ê²°ê³¼ ë°œê²¬ ê°€ëŠ¥ì„± ì ìŒ  
- MLP ì‹œëŒ€ì˜ í•œê³„  
  - 2 ê³„ì¸µì„ ë„˜ëŠ” MLPëŠ” ì˜ í•™ìŠµë˜ì§€ ì•ŠìŒ  
  - Overfitting, Local optimaê°€ ë§ì€ íƒìƒ‰ê³µê°„  
  - ë§¤ìš° ëŠë¦¬ê³  ê³ ì°¨ì› ë°ì´í„° ì²˜ë¦¬ê°€ ì˜ ë˜ì§€ ì•ŠìŒ  
    - __Vanishing gradient ë¬¸ì œì„ì´ ì•Œë ¤ì§ --> ReLU ì‚¬ìš©__  
# ReLU  
ì •ë¥˜ì„ í˜•ìœ ë‹› Rectified Linear Unit  
- ë¯¸ë¶„ì‹œ ê²½ì‚¬ë„ê°€ 0ì´ ì•„ë‹Œ ìƒìˆ˜ê°€ ëœë‹¤   
- __ì˜¤ì°¨í¬ì„ì„ ë°©ì§€í•œë‹¤__    
- í¬í™”ê°€ ë˜ì§€ ì•ŠëŠ”(non-saturating) ë¹„ì„ í˜• êµ¬ì¡°(ì‹œê·¸ëª¨ì´ë“œëŠ” í¬í™”ë˜ëŠ”(saturating) ë¹„ì„ í˜• êµ¬ì¡°)  

Regularization 
- Problem  
  - over-fitting  
- Solution  
  - Data augmentation  
    - train it on more data  
  - Determining #hidden units
  - Weight decay
  - Early stopping
  - Dropout
  - Batch normalization

# TF  
Variableì˜ ê°’ì„ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ  
-     variable.eval(), w.eval()    
ì´ˆê¸°í™”í•œ ë³€ìˆ˜ ê°’ ë³€ê²½ & ì´ˆê¸°í™” ê¸°ëŠ¥
-     tf.assign(W, [-1.])    
ì£¼ì–´ì§„ axisë¥¼ ë”°ë¼ Input ì¤‘ ê°€ì¥ í° valueì˜ indexë¥¼ return  
-     tf.argmax(input, axis=None)    
xì™€ yë¥¼ element-wiseë¡œ ë¹„êµí•˜ì—¬ í•´ë‹¹ ìœ„ì¹˜ì— ê°™ìœ¼ë©´ 1, ë‹¤ë¥´ë©´ 0ì´ í• ë‹¹  
-     tf.equal(x, y)    
tensorë¥¼ ë°˜í™˜í•´ì£¼ëŠ” operation, ì£¼ì–´ì§„ tensorë¥¼ ì£¼ì–´ì§„ dtypeìœ¼ë¡œ ë³€í™˜ì‹œì¼œ return   
-     tf.cast(x, dtype)      
  
