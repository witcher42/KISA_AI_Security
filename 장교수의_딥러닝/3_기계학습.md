## 딥러닝 : 모델의 복잡도가 큰 머신러닝  
- 특징 맵(feature map) 또는 표상(representation)을 자동으로 학습  
- LMS 최소 평균제곱오차법 : 오류 함수를 경사하강법을써서 반복적으로 최소화하는 것 
### 다층퍼셉트론 MLP 
- 퍼셉트론 + 은닉층
### 오류역전파 알고리즘 BP
- 상위층의 델타값을 이용한 하위층 뉴런의 델타값 계산
- but 학습속도가 느림, 국지적 최소점만 찾음 
### 경사도 소멸 문제 (vanishing gradient)  
- 깊은 신경망 학습시 문제점
- 오류 역전파 알고리듬으로 깊이가 깊은 심층신경망 학습시에 문제가 발생
- 역전파 오류가 입력층으로 전달됨에 따라 그래디언트가 작아지게 됨
- 가중치 매개변수가 업데이트 되지 않는 경우가 발생하여 학습이 잘 안됨
### 오차 신호 희석 문제 (gradient vanishing problem)
- Sigmoid는 순환신경망에서 사용되면 근방의 정보만 반영하여 학습하여 데이터의 순서를 학습하지 못함
- 오차 신호 희석 문제를 해결하기 위해서 ReLU를 활성화 함수로 활용  
- 해결
  - 장단기 메모리(Long-Short Term Memory, LSTM) 신경망 모델
    - Gate를 도입하여 cell state를 제어
    -  LSTM 내부 구조에는 sigmoid, tanh가 들어있음  
### 품사 추론 LSTM 모델
- 문장이 주어지면, 각 단어의 품사를 추론하는 모델 설계  
- 이전 단어들을 기억할 수 있는 LSTM 모델이 적합
### Word2Vec
- 한 문장에 있는 단어들이 similar context라 간주

### 컨볼루션 커널 
- 구조 설계를 통해 모델 복잡도를 줄임  
